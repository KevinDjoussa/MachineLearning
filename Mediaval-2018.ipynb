{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCuhFVrLX2Fg"
   },
   "source": [
    "## Connect to file system\n",
    "\n",
    "To run the scrpt as it is, you need to create following folders in the home directory of google drive. \n",
    "\n",
    "\n",
    "*   /content/drive/My Drive/CA684_Assignment - shortcut to resource folderr\n",
    "*   /content/drive/My Drive/upwork/\n",
    "*   /content/drive/My Drive/upwork/test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RzuRRP1CXu0E",
    "outputId": "fe75169c-7d20-4a7f-9117-9d312ea4aa62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive/')\n",
    "os.chdir('/content/drive/My Drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stVNi0wvYRtJ"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7vTRGF-RX8pG",
    "outputId": "a537efc7-3fef-4a8f-e9a9-33fde236fa4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyprind in /usr/local/lib/python3.6/dist-packages (2.11.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pFRChGIRwVVV",
    "outputId": "8efe740e-6090-4fe6-80d2-55596ef8dff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mG8RWC1YIrz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import pyprind\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4gkj9mydEu9"
   },
   "source": [
    "# 1. Loading ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rVJNT1XdKsG"
   },
   "outputs": [],
   "source": [
    "# load ground truth labels\n",
    "label_path = './CA684_Assignment/Dev-set/Ground-truth/'\n",
    "labels=pd.read_csv(label_path+'ground-truth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CdPeMlrZu-k"
   },
   "source": [
    "# 2. Loading data\n",
    "\n",
    "This section contains all the functions to extract features from the goodle drive as pandas dataframes. To reduce the loading time, dataframes are saved seperately on the gdrive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uD80eFIMmVun"
   },
   "source": [
    "### 2.1 Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ms37FpSCSKIo"
   },
   "outputs": [],
   "source": [
    "# load labels and captions\n",
    "def read_caps(fname):\n",
    "    \"\"\"Load the captions into a dataframe\"\"\"\n",
    "    vn = []\n",
    "    cap = []\n",
    "    df = pd.DataFrame();\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            vn.append(pairs[0])\n",
    "            cap.append(pairs[1])\n",
    "        df['video']=vn\n",
    "        df['caption']=cap\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5BXHqcirxsN"
   },
   "outputs": [],
   "source": [
    "# Common tokenizer,  is trained from the text in training set\n",
    "# tokenizer used for validation set and test set\n",
    "cap_path = './CA684_Assignment/Dev-set/Captions/dev-set_video-captions.txt'\n",
    "df_cap=read_caps(cap_path)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(df_cap.caption.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEvUs6apYQ-X"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_captions(pkl_dir, raw_dir):\n",
    "  '''\n",
    "  If caption features are saved before return the dataframe\n",
    "  else read text file, save as a dataframe and return it\n",
    "  '''\n",
    "  try:\n",
    "    features_captions = pd.read_pickle(pkl_dir)\n",
    "    return features_captions\n",
    "  except:\n",
    "    cap_path = raw_dir\n",
    "    df_cap=read_caps(cap_path)\n",
    "\n",
    "    captions_one_hot_res = tokenizer.texts_to_matrix(list(df_cap.caption.values),mode='binary')\n",
    "    features_captions = pd.DataFrame(np.concatenate((df_cap.video.values.reshape((-1,1)), captions_one_hot_res), axis=1))\n",
    "    features_captions.columns = ['video'] + ['caption_token_{}'.format(i) for i in range(captions_one_hot_res.shape[1])]\n",
    "    features_captions.to_pickle(pkl_dir)\n",
    "    return features_captions\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyDL2yDiqhOt"
   },
   "source": [
    "### 2.2 Inception features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGXGOWm1twn4"
   },
   "outputs": [],
   "source": [
    "def parse_inception_feature(s):\n",
    "    pairs = s.strip().split(' ')\n",
    "    pairs = [i.split(':') for i in pairs]\n",
    "    return {int(k): float(v) for k, v in pairs}\n",
    "\n",
    "def expand_inception_feature(d):\n",
    "  feature = np.zeros(1000)\n",
    "  for k, v in d.items():\n",
    "    feature[k] = v\n",
    "  return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13m6-6Y0ucRY"
   },
   "outputs": [],
   "source": [
    "def get_inception_features_layer(layer_num, pkl_dir, raw_dir):\n",
    "  '''\n",
    "  If inception features are saved before return the dataframe\n",
    "  else read text file, save as a dataframe and return it\n",
    "  layer num - the frame number features needed\n",
    "  '''\n",
    "  try:\n",
    "    features_inception_0 = pd.read_pickle(pkl_dir.format(layer_num))\n",
    "    return features_inception_0\n",
    "  except:\n",
    "    inception_features_dict = []\n",
    "    inception_path = Path(raw_dir)\n",
    "    for file in tqdm(list(inception_path.glob('*-{}.txt'.format(layer_num)))):\n",
    "      # only process first frame of each video\n",
    "      key = file.with_suffix('.webm').name.replace('-{}'.format(layer_num), '')\n",
    "      inception_features_dict.append([key] + expand_inception_feature(\n",
    "        parse_inception_feature(\n",
    "          file.open().read())).tolist())\n",
    "\n",
    "    features_inception_0 = pd.DataFrame(inception_features_dict)\n",
    "    features_inception_0.columns = ['video'] + ['inception_{}_{}'.format(layer_num, i) for i in range(1000)]\n",
    "    features_inception_0.to_pickle(pkl_dir.format(layer_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqIm8Xdhxpun"
   },
   "source": [
    "### 2.3 C3D features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlFpASV6x2Nd"
   },
   "outputs": [],
   "source": [
    "def read_C3D(fname):\n",
    "    \"\"\"Scan vectors from file\"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            C3D =[float(item) for item in line.split()] # convert to float type, using default separator\n",
    "    return C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nihMzuBOyAcP"
   },
   "outputs": [],
   "source": [
    "def get_c3d_features(pkl_dir, raw_dir):\n",
    "  '''\n",
    "  If c3d features are saved before return the dataframe\n",
    "  else read text file, save as a dataframe and return it\n",
    "  '''\n",
    "  try:\n",
    "    features_c3d = pd.read_pickle(pkl_dir)\n",
    "    return features_c3d\n",
    "  except:\n",
    "    c3d_features_dict = []\n",
    "    c3d_path = Path(raw_dir)\n",
    "    for file in tqdm(list(c3d_path.glob('*.txt'))):\n",
    "      # only process first frame of each video\n",
    "      key = file.with_suffix('.webm').name\n",
    "      c3d_features_dict.append([key] + read_C3D(file))\n",
    "\n",
    "    features_c3d = pd.DataFrame(c3d_features_dict)\n",
    "    features_c3d.columns = ['video'] + ['c3d_{}'.format(i) for i in range(101)]\n",
    "    features_c3d.to_pickle(pkl_dir)\n",
    "    return features_c3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccOhnf3op5IA"
   },
   "source": [
    "### 2.4 HMP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7syQJR04qo9Q"
   },
   "outputs": [],
   "source": [
    "def read_HMP(fname):\n",
    "    \"\"\"Scan HMP(Histogram of Motion Patterns) features from file\"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            pairs=line.split()\n",
    "            HMP_temp = { int(p.split(':')[0]) : float(p.split(':')[1]) for p in pairs}\n",
    "    # there are 6075 bins, fill zeros\n",
    "    HMP = np.zeros(6075)\n",
    "    for idx in HMP_temp.keys():\n",
    "        HMP[idx-1] = HMP_temp[idx]            \n",
    "    return HMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4RI8z4Ep2eV"
   },
   "outputs": [],
   "source": [
    "def get_hmp_features(pkl_dir, raw_dir):\n",
    "  '''\n",
    "  If hmp features are saved before return the dataframe\n",
    "  else read text file, save as a dataframe and return it\n",
    "  '''\n",
    "  try:\n",
    "    features_hmp = pd.read_pickle(pkl_dir)\n",
    "    return features_hmp\n",
    "  except:\n",
    "    hmp_features_dict = []\n",
    "    hmp_path = Path(raw_dir)\n",
    "    for file in tqdm(list(hmp_path.glob('*.txt'))):\n",
    "      # only process first frame of each video\n",
    "      key = file.with_suffix('.webm').name\n",
    "      hmp_features_dict.append([key] + read_HMP(file).tolist())\n",
    "\n",
    "    features_hmp = pd.DataFrame(hmp_features_dict)\n",
    "    features_hmp.columns = ['video'] + ['hmp_{}'.format(i) for i in range(6075)]\n",
    "    features_hmp.to_pickle(pkl_dir)\n",
    "    return features_hmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWXdATTDuKhY"
   },
   "source": [
    "### 2.5 Color historgram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgRo4F8juUbz"
   },
   "outputs": [],
   "source": [
    "def read_ColorHistogram(fname):\n",
    "    \"\"\"Scan Color Histogram from file\n",
    "    Input file contains RGB histogram,\n",
    "    Return a matrix of (3,256)\"\"\"\n",
    "    RGB_Hist = np.zeros((3,256))\n",
    "    with open(fname) as f:\n",
    "        i_l = 0 # line index\n",
    "        for line in f:\n",
    "            pairs = line.split()\n",
    "            hist_dict = {int(p.split(':')[0]):float(p.split(':')[1]) for p in pairs}\n",
    "            for idx in hist_dict.keys():\n",
    "                RGB_Hist[i_l,idx] = hist_dict[idx]\n",
    "            i_l += 1\n",
    "    return RGB_Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFqZw-Luvk83"
   },
   "outputs": [],
   "source": [
    "def hist_features_layer(layer_num, pkl_dir, raw_dir):\n",
    "  '''\n",
    "  If color histogram features are saved before return the dataframe\n",
    "  else read text file, save as a dataframe and return it\n",
    "  '''\n",
    "  try:\n",
    "    features_hist_0 = pd.read_pickle(pkl_dir.format(layer_num))\n",
    "    return features_hist_0\n",
    "  except:\n",
    "    hist_features_dict = []\n",
    "    hist_path = Path(raw_dir)\n",
    "    for file in tqdm(list(hist_path.glob('*-{}.txt'.format(layer_num)))):\n",
    "      # only process first frame of each video\n",
    "      key = file.with_suffix('.webm').name.replace('-{}'.format(layer_num), '')\n",
    "      hist_features_dict.append([key] + read_ColorHistogram(file).flatten().tolist())\n",
    "\n",
    "    features_hist = pd.DataFrame(hist_features_dict)\n",
    "    features_hist.columns = ['video'] + ['hist_{}_{}'.format(layer_num, i) for i in range(768)]\n",
    "    features_hist.to_pickle(pkl_dir.format(layer_num))\n",
    "    return features_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YNXJJHbQGS6"
   },
   "source": [
    "### 2.6 Load features\n",
    "\n",
    "In this section data is loaded fromt the functions implemented in previous section. Data form test and training folders are loaded and saved seperrately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNOZUJzEQJ2D"
   },
   "outputs": [],
   "source": [
    "features_captions = get_features_captions('./upwork/features_captions.pkl', './CA684_Assignment/Dev-set/Captions/dev-set_video-captions.txt')\n",
    "test_features_captions = get_features_captions('./upwork/test/features_captions.pkl', './CA684_Assignment/Test-set/Captions_test/test-set-1_video-captions.txt')\n",
    "\n",
    "features_inception_0 = get_inception_features_layer(0, './upwork/features_inception_{}.pkl', './CA684_Assignment/Dev-set/InceptionV3')\n",
    "test_features_inception_0 = get_inception_features_layer(0, './upwork/test/features_inception_{}.pkl', './CA684_Assignment/Test-set/Inception_test')\n",
    "\n",
    "features_inception_56 = get_inception_features_layer(56, './upwork/features_inception_{}.pkl', './CA684_Assignment/Dev-set/InceptionV3')\n",
    "test_features_inception_56 = get_inception_features_layer(56, './upwork/test/features_inception_{}.pkl', './CA684_Assignment/Test-set/Inception_test')\n",
    "\n",
    "features_inception_112 = get_inception_features_layer(112, './upwork/features_inception_{}.pkl', './CA684_Assignment/Dev-set/InceptionV3')\n",
    "test_features_inception_112 = get_inception_features_layer(112, './upwork/test/features_inception_{}.pkl', './CA684_Assignment/Test-set/Inception_test')\n",
    "\n",
    "features_c3d = get_c3d_features('./upwork/features_c3d.pkl', './CA684_Assignment/Dev-set/C3D')\n",
    "test_features_c3d = get_c3d_features('./upwork/test/features_c3d.pkl', './CA684_Assignment/Test-set/C3D_test')\n",
    "\n",
    "features_hmp = get_hmp_features('./upwork/features_hmp.pkl', './CA684_Assignment/Dev-set/HMP')\n",
    "test_features_hmp = get_hmp_features('./upwork/test/features_hmp.pkl', './CA684_Assignment/Test-set/HMP_test')\n",
    "\n",
    "features_hist_0 = hist_features_layer(0, './upwork/features_hist_{}.pkl', './CA684_Assignment/Dev-set/ColorHistogram')\n",
    "test_features_hist_0 = hist_features_layer(0, './upwork/test/features_hist_{}.pkl', './CA684_Assignment/Test-set/ColorHistogram_test')\n",
    "\n",
    "features_hist_56 = hist_features_layer(56, './upwork/features_hist_{}.pkl', './CA684_Assignment/Dev-set/ColorHistogram')\n",
    "test_features_hist_56 = hist_features_layer(56, './upwork/test/features_hist_{}.pkl', './CA684_Assignment/Test-set/ColorHistogram_test')\n",
    "\n",
    "features_hist_112 = hist_features_layer(112, './upwork/features_hist_{}.pkl', './CA684_Assignment/Dev-set/ColorHistogram')\n",
    "test_features_hist_112 = hist_features_layer(112, './upwork/test/features_hist_{}.pkl', './CA684_Assignment/Test-set/ColorHistogram_test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z01ESuQ4dUHa"
   },
   "source": [
    "# 3. Training data\n",
    "\n",
    "Split and preprocess the data in this section. Before preprocessing all the features are merged into a single pandas dataframe. Then Split it to 0.2 ratio and finally min max scaler used to scale between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0086u6w8Ee8r"
   },
   "outputs": [],
   "source": [
    "# merge all the feature dataframes using 'video' name index\n",
    "all_features = [features_captions, features_c3d, features_hmp, features_inception_0, features_inception_56, features_inception_112, features_hist_0, features_hist_112]\n",
    "all_features_columns = []\n",
    "for df in all_features:\n",
    "  df.set_index('video', inplace=True)\n",
    "  all_features_columns += df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWasyJUaNz7F"
   },
   "outputs": [],
   "source": [
    "# index ground truth by video name\n",
    "labels.set_index('video', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuDOMMtIN-FF"
   },
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "full_dataset_pd = labels.copy()\n",
    "for df in all_features:\n",
    "  full_dataset_pd = pd.merge(full_dataset_pd, df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6J7VHCf7b503"
   },
   "outputs": [],
   "source": [
    "# split dataset into validation and training sets with 0.2 rato\n",
    "Y = full_dataset_pd[['short-term_memorability','long-term_memorability']].values # targets\n",
    "X = full_dataset_pd[all_features_columns].values.astype(np.float32)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42) # random state for reproducability\n",
    "len_token = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abNph1DxrmT"
   },
   "outputs": [],
   "source": [
    "# scale dataset to range [0,1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iHf4osIiaTf"
   },
   "source": [
    "# 4. Evaluate\n",
    "\n",
    "Script to evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVdWl-oojsZe"
   },
   "outputs": [],
   "source": [
    "def Get_score(Y_pred,Y_true):\n",
    "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
    "    Y_pred = np.squeeze(Y_pred)\n",
    "    Y_true = np.squeeze(Y_true)\n",
    "    if Y_pred.shape != Y_true.shape:\n",
    "        print('Input shapes don\\'t match!')\n",
    "    else:\n",
    "        if len(Y_pred.shape) == 1:\n",
    "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
    "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
    "            print('The Spearman\\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])\n",
    "        else:\n",
    "            for ii in range(Y_pred.shape[1]):\n",
    "                Get_score(Y_pred[:,ii],Y_true[:,ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_wHV2Jyo9-4"
   },
   "source": [
    "# 5. Gradient boosting model\n",
    "\n",
    "Two LGBM models are trained seperately for long term and short term memorability prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "G2c_D5GjpGm1",
    "outputId": "de7104a8-9d0b-44a7-a035-1cc4a075847d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:123: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.00611871\tvalid_0's l2: 0.00611871\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.00603924\tvalid_0's l2: 0.00603924\n",
      "[3]\tvalid_0's l2: 0.00598137\tvalid_0's l2: 0.00598137\n",
      "[4]\tvalid_0's l2: 0.00593721\tvalid_0's l2: 0.00593721\n",
      "[5]\tvalid_0's l2: 0.00587094\tvalid_0's l2: 0.00587094\n",
      "[6]\tvalid_0's l2: 0.00582673\tvalid_0's l2: 0.00582673\n",
      "[7]\tvalid_0's l2: 0.00578061\tvalid_0's l2: 0.00578061\n",
      "[8]\tvalid_0's l2: 0.00574117\tvalid_0's l2: 0.00574117\n",
      "[9]\tvalid_0's l2: 0.00570729\tvalid_0's l2: 0.00570729\n",
      "[10]\tvalid_0's l2: 0.00566665\tvalid_0's l2: 0.00566665\n",
      "[11]\tvalid_0's l2: 0.0056238\tvalid_0's l2: 0.0056238\n",
      "[12]\tvalid_0's l2: 0.00560763\tvalid_0's l2: 0.00560763\n",
      "[13]\tvalid_0's l2: 0.00557312\tvalid_0's l2: 0.00557312\n",
      "[14]\tvalid_0's l2: 0.00554553\tvalid_0's l2: 0.00554553\n",
      "[15]\tvalid_0's l2: 0.00550382\tvalid_0's l2: 0.00550382\n",
      "[16]\tvalid_0's l2: 0.00548003\tvalid_0's l2: 0.00548003\n",
      "[17]\tvalid_0's l2: 0.00545648\tvalid_0's l2: 0.00545648\n",
      "[18]\tvalid_0's l2: 0.00543898\tvalid_0's l2: 0.00543898\n",
      "[19]\tvalid_0's l2: 0.00541985\tvalid_0's l2: 0.00541985\n",
      "[20]\tvalid_0's l2: 0.00540246\tvalid_0's l2: 0.00540246\n",
      "[21]\tvalid_0's l2: 0.00537666\tvalid_0's l2: 0.00537666\n",
      "[22]\tvalid_0's l2: 0.00536573\tvalid_0's l2: 0.00536573\n",
      "[23]\tvalid_0's l2: 0.00534456\tvalid_0's l2: 0.00534456\n",
      "[24]\tvalid_0's l2: 0.00532558\tvalid_0's l2: 0.00532558\n",
      "[25]\tvalid_0's l2: 0.00531318\tvalid_0's l2: 0.00531318\n",
      "[26]\tvalid_0's l2: 0.00531026\tvalid_0's l2: 0.00531026\n",
      "[27]\tvalid_0's l2: 0.00529457\tvalid_0's l2: 0.00529457\n",
      "[28]\tvalid_0's l2: 0.00527657\tvalid_0's l2: 0.00527657\n",
      "[29]\tvalid_0's l2: 0.00526312\tvalid_0's l2: 0.00526312\n",
      "[30]\tvalid_0's l2: 0.00525784\tvalid_0's l2: 0.00525784\n",
      "[31]\tvalid_0's l2: 0.00525207\tvalid_0's l2: 0.00525207\n",
      "[32]\tvalid_0's l2: 0.00523858\tvalid_0's l2: 0.00523858\n",
      "[33]\tvalid_0's l2: 0.00521944\tvalid_0's l2: 0.00521944\n",
      "[34]\tvalid_0's l2: 0.00521151\tvalid_0's l2: 0.00521151\n",
      "[35]\tvalid_0's l2: 0.00520926\tvalid_0's l2: 0.00520926\n",
      "[36]\tvalid_0's l2: 0.00520526\tvalid_0's l2: 0.00520526\n",
      "[37]\tvalid_0's l2: 0.00519881\tvalid_0's l2: 0.00519881\n",
      "[38]\tvalid_0's l2: 0.005196\tvalid_0's l2: 0.005196\n",
      "[39]\tvalid_0's l2: 0.0052016\tvalid_0's l2: 0.0052016\n",
      "[40]\tvalid_0's l2: 0.00519915\tvalid_0's l2: 0.00519915\n",
      "[41]\tvalid_0's l2: 0.00519527\tvalid_0's l2: 0.00519527\n",
      "[42]\tvalid_0's l2: 0.00519162\tvalid_0's l2: 0.00519162\n",
      "[43]\tvalid_0's l2: 0.00518135\tvalid_0's l2: 0.00518135\n",
      "[44]\tvalid_0's l2: 0.00517274\tvalid_0's l2: 0.00517274\n",
      "[45]\tvalid_0's l2: 0.00515733\tvalid_0's l2: 0.00515733\n",
      "[46]\tvalid_0's l2: 0.00515445\tvalid_0's l2: 0.00515445\n",
      "[47]\tvalid_0's l2: 0.0051555\tvalid_0's l2: 0.0051555\n",
      "[48]\tvalid_0's l2: 0.00514467\tvalid_0's l2: 0.00514467\n",
      "[49]\tvalid_0's l2: 0.00515165\tvalid_0's l2: 0.00515165\n",
      "[50]\tvalid_0's l2: 0.00514178\tvalid_0's l2: 0.00514178\n",
      "[51]\tvalid_0's l2: 0.00513616\tvalid_0's l2: 0.00513616\n",
      "[52]\tvalid_0's l2: 0.00513196\tvalid_0's l2: 0.00513196\n",
      "[53]\tvalid_0's l2: 0.00512994\tvalid_0's l2: 0.00512994\n",
      "[54]\tvalid_0's l2: 0.00511806\tvalid_0's l2: 0.00511806\n",
      "[55]\tvalid_0's l2: 0.00511764\tvalid_0's l2: 0.00511764\n",
      "[56]\tvalid_0's l2: 0.00510567\tvalid_0's l2: 0.00510567\n",
      "[57]\tvalid_0's l2: 0.00510045\tvalid_0's l2: 0.00510045\n",
      "[58]\tvalid_0's l2: 0.00510033\tvalid_0's l2: 0.00510033\n",
      "[59]\tvalid_0's l2: 0.00509554\tvalid_0's l2: 0.00509554\n",
      "[60]\tvalid_0's l2: 0.00509999\tvalid_0's l2: 0.00509999\n",
      "[61]\tvalid_0's l2: 0.00509749\tvalid_0's l2: 0.00509749\n",
      "[62]\tvalid_0's l2: 0.00509791\tvalid_0's l2: 0.00509791\n",
      "[63]\tvalid_0's l2: 0.00509535\tvalid_0's l2: 0.00509535\n",
      "[64]\tvalid_0's l2: 0.0050928\tvalid_0's l2: 0.0050928\n",
      "[65]\tvalid_0's l2: 0.00508599\tvalid_0's l2: 0.00508599\n",
      "[66]\tvalid_0's l2: 0.00508199\tvalid_0's l2: 0.00508199\n",
      "[67]\tvalid_0's l2: 0.0050807\tvalid_0's l2: 0.0050807\n",
      "[68]\tvalid_0's l2: 0.0050793\tvalid_0's l2: 0.0050793\n",
      "[69]\tvalid_0's l2: 0.00508253\tvalid_0's l2: 0.00508253\n",
      "[70]\tvalid_0's l2: 0.00508177\tvalid_0's l2: 0.00508177\n",
      "[71]\tvalid_0's l2: 0.00507506\tvalid_0's l2: 0.00507506\n",
      "[72]\tvalid_0's l2: 0.0050733\tvalid_0's l2: 0.0050733\n",
      "[73]\tvalid_0's l2: 0.00507411\tvalid_0's l2: 0.00507411\n",
      "[74]\tvalid_0's l2: 0.00507252\tvalid_0's l2: 0.00507252\n",
      "[75]\tvalid_0's l2: 0.00506852\tvalid_0's l2: 0.00506852\n",
      "[76]\tvalid_0's l2: 0.00506761\tvalid_0's l2: 0.00506761\n",
      "[77]\tvalid_0's l2: 0.00506151\tvalid_0's l2: 0.00506151\n",
      "[78]\tvalid_0's l2: 0.00505508\tvalid_0's l2: 0.00505508\n",
      "[79]\tvalid_0's l2: 0.00505001\tvalid_0's l2: 0.00505001\n",
      "[80]\tvalid_0's l2: 0.00505003\tvalid_0's l2: 0.00505003\n",
      "[81]\tvalid_0's l2: 0.00504927\tvalid_0's l2: 0.00504927\n",
      "[82]\tvalid_0's l2: 0.00504609\tvalid_0's l2: 0.00504609\n",
      "[83]\tvalid_0's l2: 0.00504421\tvalid_0's l2: 0.00504421\n",
      "[84]\tvalid_0's l2: 0.00504099\tvalid_0's l2: 0.00504099\n",
      "[85]\tvalid_0's l2: 0.00504413\tvalid_0's l2: 0.00504413\n",
      "[86]\tvalid_0's l2: 0.00504095\tvalid_0's l2: 0.00504095\n",
      "[87]\tvalid_0's l2: 0.00503965\tvalid_0's l2: 0.00503965\n",
      "[88]\tvalid_0's l2: 0.00504032\tvalid_0's l2: 0.00504032\n",
      "[89]\tvalid_0's l2: 0.00503919\tvalid_0's l2: 0.00503919\n",
      "[90]\tvalid_0's l2: 0.00503635\tvalid_0's l2: 0.00503635\n",
      "[91]\tvalid_0's l2: 0.0050342\tvalid_0's l2: 0.0050342\n",
      "[92]\tvalid_0's l2: 0.00503446\tvalid_0's l2: 0.00503446\n",
      "[93]\tvalid_0's l2: 0.0050342\tvalid_0's l2: 0.0050342\n",
      "[94]\tvalid_0's l2: 0.00503596\tvalid_0's l2: 0.00503596\n",
      "[95]\tvalid_0's l2: 0.0050321\tvalid_0's l2: 0.0050321\n",
      "[96]\tvalid_0's l2: 0.00503392\tvalid_0's l2: 0.00503392\n",
      "[97]\tvalid_0's l2: 0.00503766\tvalid_0's l2: 0.00503766\n",
      "[98]\tvalid_0's l2: 0.00504049\tvalid_0's l2: 0.00504049\n",
      "[99]\tvalid_0's l2: 0.00504055\tvalid_0's l2: 0.00504055\n",
      "[100]\tvalid_0's l2: 0.00503633\tvalid_0's l2: 0.00503633\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's l2: 0.0050321\tvalid_0's l2: 0.0050321\n",
      "The Spearman's correlation coefficient is: 0.409\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting regressor \n",
    "# gbm_0 is for the short term memorability\n",
    "gbm_0 = lgb.LGBMRegressor(num_leaves=55,\n",
    "                        learning_rate=0.05,\n",
    "                        early_stopping_rounds=5,\n",
    "                        n_estimators=100)\n",
    "gbm_0.fit(X_train, Y_train[:, 0],\n",
    "        eval_set=[(X_test, Y_test[:, 0])],\n",
    "        eval_metric='l2',\n",
    "        early_stopping_rounds=5)\n",
    "pred = gbm_0.predict(X_test)\n",
    "Get_score(pred, Y_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "W3endpDtqdv9",
    "outputId": "6665e7e5-9571-4664-91d3-877a57e9af70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.0218202\tvalid_0's l1: 0.118063\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.0217785\tvalid_0's l1: 0.117918\n",
      "[3]\tvalid_0's l2: 0.0217243\tvalid_0's l1: 0.117757\n",
      "[4]\tvalid_0's l2: 0.0216927\tvalid_0's l1: 0.117605\n",
      "[5]\tvalid_0's l2: 0.0216572\tvalid_0's l1: 0.117474\n",
      "[6]\tvalid_0's l2: 0.021621\tvalid_0's l1: 0.117436\n",
      "[7]\tvalid_0's l2: 0.0215804\tvalid_0's l1: 0.11729\n",
      "[8]\tvalid_0's l2: 0.021567\tvalid_0's l1: 0.117227\n",
      "[9]\tvalid_0's l2: 0.0215521\tvalid_0's l1: 0.117231\n",
      "[10]\tvalid_0's l2: 0.0215815\tvalid_0's l1: 0.117317\n",
      "[11]\tvalid_0's l2: 0.0216079\tvalid_0's l1: 0.117358\n",
      "[12]\tvalid_0's l2: 0.0216016\tvalid_0's l1: 0.117292\n",
      "[13]\tvalid_0's l2: 0.021612\tvalid_0's l1: 0.117321\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's l2: 0.021567\tvalid_0's l1: 0.117227\n",
      "The Spearman's correlation coefficient is: 0.188\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting regressor \n",
    "# gbm_0 is for the long term memorability\n",
    "\n",
    "gbm_1 = lgb.LGBMRegressor(num_leaves=31,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=20)\n",
    "gbm_1.fit(X_train, Y_train[:, 1],\n",
    "        eval_set=[(X_test, Y_test[:, 1])],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "pred = gbm_0.predict(X_test)\n",
    "Get_score(pred, Y_test[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yieVPbg1BifR"
   },
   "source": [
    "# 6. Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "08jFJG83_oFw",
    "outputId": "156d5729-91c3-47ca-d15a-cfdcbf93f655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.409\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=120).fit(X_train, Y_train[:,0])\n",
    "pred = reg.predict(X_test)\n",
    "Get_score(pred, Y_test_new[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dQoMcVk_GRg5",
    "outputId": "ea8a373d-74da-4205-d45e-1a2283afc0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman's correlation coefficient is: 0.408\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=[ 1, 10, 100, 1000, 10000]).fit(X_train, Y_train[:,0])\n",
    "pred = reg.predict(X_test)\n",
    "Get_score(pred, Y_test_new[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0WgsKYHJ_T9"
   },
   "source": [
    "# 7. Predict\n",
    "\n",
    "In this section data from the test folder of the dataset is processed. Then feed them to the previously trained LGBM models and result saved to a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M__H-aUOIZFT"
   },
   "outputs": [],
   "source": [
    "# Merge all the test dataset features into variable 'test_full_dataset_pd'\n",
    "test_all_features = [test_features_captions, test_features_c3d, test_features_hmp, \n",
    "                test_features_inception_0, test_features_inception_56, \n",
    "                test_features_inception_112, test_features_hist_0, \n",
    "                test_features_hist_112]\n",
    "\n",
    "# set the index for all the features\n",
    "test_all_features_columns = []\n",
    "for df in test_all_features:\n",
    "  df.set_index('video', inplace=True)\n",
    "  test_all_features_columns += df.columns.to_list()\n",
    "\n",
    "# merging features\n",
    "test_full_dataset_pd = test_all_features[0].copy()\n",
    "for df in test_all_features[1:]:\n",
    "  test_full_dataset_pd = pd.merge(test_full_dataset_pd, df, left_index=True, right_index=True)\n",
    "\n",
    "test_X = test_full_dataset_pd[test_all_features_columns].values.astype(np.float32)\n",
    "# scaling the dataset\n",
    "test_X = min_max_scaler.transform(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fo5R2aoKKcUX"
   },
   "outputs": [],
   "source": [
    "short_term_pred = gbm_0.predict(test_X)\n",
    "long_term_pred = gbm_1.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zosb5O6QMbRv"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'short-term_memorability':short_term_pred, 'long-term_memorability':long_term_pred}).to_csv('./upwork/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjf_MlaciZ1x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JyDL2yDiqhOt",
    "GqIm8Xdhxpun",
    "ccOhnf3op5IA",
    "yWXdATTDuKhY"
   ],
   "machine_shape": "hm",
   "name": "upwork_ml_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
